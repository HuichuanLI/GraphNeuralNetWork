{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa1597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a node's using 'convolutional' GraphSage approach\n",
    "    \"\"\"\n",
    "    def __init__(self, features, feature_dim, \n",
    "            embed_dim, adj_lists, aggregator,\n",
    "            num_sample=10,\n",
    "            base_model=None, gcn=False, cuda=False, \n",
    "            feature_transform=False): \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        # 变换前的hidden_size/维度\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        # 即邻居聚合后的mebedding: agg1 = MeanAggregator(features, cuda=True)\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "\n",
    "        # 默认False, model.py里面设置成True\n",
    "        self.gcn = gcn\n",
    "        # 变换后的hidden_size/维度\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        # 矩阵W维度 = 变换后维度 * 变换前维度\n",
    "        # 其中gcn表示是否拼接，如果拼接的话由于是\"自身向量||邻居聚合向量\", 所以维度为2倍\n",
    "        self.weight = nn.Parameter(\n",
    "                torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "\n",
    "        nodes     -- list of nodes\n",
    "        \"\"\"\n",
    "        # 调用aggregator.py文件中的MeanAggregator class的forward函数，得到聚合邻居的信息\n",
    "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], \n",
    "                self.num_sample)\n",
    "        if not self.gcn:\n",
    "            if self.cuda:\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else:\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "            # 将自身和聚合邻居的向量拼接, algorithm 1 line 5的拼接部分\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        else:\n",
    "            # 只用聚合邻居的向量来表示，不用自身信息, algorithm 1 line 5的拼接部分\n",
    "            combined = neigh_feats\n",
    "        # 送入到神经网络，algorithm 1 line 5乘以矩阵W\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        # 经过一层GNN layer后的点的embedding，维度为embed_dim * nodes\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7747c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Set of modules for aggregating embeddings of neighbors.\n",
    "\"\"\"\n",
    "\n",
    "# 实现聚合类，对邻居信息进行AGGREGATE\n",
    "class MeanAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregates a node's embeddings using mean of neighbors' embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, features, cuda=False, gcn=False): \n",
    "        \"\"\"\n",
    "        Initializes the aggregator for a specific graph.\n",
    "\n",
    "        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
    "        cuda -- whether to use GPU\n",
    "        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
    "        \"\"\"\n",
    "\n",
    "        super(MeanAggregator, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.gcn = gcn\n",
    "\n",
    "\n",
    "    def forward(self, nodes, to_neighs, num_sample=10):\n",
    "        \"\"\"\n",
    "        # batch中的点的列表\n",
    "        nodes --- list of nodes in a batch\n",
    "        # batch中每个点对应的邻居集合\n",
    "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
    "        num_sample --- number of neighbors to sample. No sampling if None.\n",
    "        \"\"\"\n",
    "        # Local pointers to functions (speed hack)\n",
    "        _set = set\n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            # 首先对每一个节点的邻居集合neigh进行遍历，判断一下已有邻居数和采样数大小，多于采样数进行抽样\n",
    "            # 对一个batch中的每一个节点的邻接点set进行sample\n",
    "            samp_neighs = [_set(_sample(to_neigh, \n",
    "                            num_sample,\n",
    "                            )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "\n",
    "        # 将自己也作为自己的邻居点 (类似于GCN里面的A + I的操作)\n",
    "        if self.gcn:\n",
    "            samp_neighs = [samp_neigh + set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "        # *拆解列表后，转为为多个独立的元素作为参数给union，union函数进行去重合并\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        # 节点标号不一定都是从0开始的，创建一个字典，key为节点ID，value为节点序号 (old id到new id的转换，为下面列切片做准备)\n",
    "        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
    "        # print(len(nodes), len(unique_nodes), len(samp_neighs))\n",
    "\n",
    "        # 构建缩小的邻接矩阵，即这个batch所用到的点所构成的小的邻接矩阵\n",
    "        # nodes表示batch内的节点，unique_nodes表示batch内的节点用到的所有邻居节点，unique_nodes > nodes\n",
    "        # len(samp_neighs)是这个batch的大小，即nodes数量，创建一个nodes * unique_nodes大小的邻接矩阵\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "        # 列切片, 遍历每一个邻居集合的每一个元素，并且通过unique_nodes(old id)获取到节点对应的序号\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "        # 行切片, 比如samp_neighs = [{3,5,9}, {2,8}, {2}]，行切片为[0,0,0,1,1,2]\n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        # 利用切片创建图的邻接矩阵\n",
    "        # 即(row_indices[i], column_indices[i])对应的位置为1\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "\n",
    "        # 构造邻接矩阵\n",
    "        # 统计每一个节点的邻居数量\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        # 归一化(除以邻居数量)\n",
    "        mask = mask.div(num_neigh)\n",
    "        # embed_matrix: [n, m]\n",
    "        # n: unique_nodes\n",
    "        # m: dim\n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        # mask是nodes * unique_nodes大小的邻接矩阵, embed_matrix是unique_nodes * hid_size的特征矩阵\n",
    "        # 即A * X, 这里A是邻接矩阵， X是特征矩阵，这里一系列的操作是按batch训练需要采样出一个局部的A\n",
    "        to_feats = mask.mm(embed_matrix)\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d037fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ca7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        # 这里面赋值为enc2(经过两层GNN)\n",
    "        self.enc = enc\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "        # 全连接参数矩阵，映射到labels num_classes维度做分类\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        # embeds实际是我们两层GNN后的输出nodes embedding\n",
    "        embeds = self.enc(nodes)\n",
    "        # 最后将nodes * hidden size 映射到 nodes * num_classes(= 7)之后做softmax计算cross entropy\n",
    "        scores = self.weight.mm(embeds)\n",
    "        return scores.t()\n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        # 钱箱传播\n",
    "        scores = self.forward(nodes)\n",
    "        # 定义的cross entropy\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5abd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora():\n",
    "    # 点的数量\n",
    "    num_nodes = 2708\n",
    "    # 特征数量\n",
    "    num_feats = 1433\n",
    "    # 构建特征矩阵\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    # 构建节点的ground truth标签\n",
    "    labels = np.empty((num_nodes,1), dtype=np.int64)\n",
    "    # 做一个点的id映射\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "\n",
    "    # 读节点特征\n",
    "    # cora.content第一列是node id, 中间为点的特征，最后一列为label\n",
    "    # with open(\"cora/cora.content\") as fp:\n",
    "    with open(\"./cora/cora.content\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            # 特征，全部转换成float类型\n",
    "            # feat_data[i,:] = map(float, info[1:-1])\n",
    "            tmp = []\n",
    "            for ss in info[1:-1]:\n",
    "                tmp.append(float(ss))\n",
    "            feat_data[i,:] = tmp\n",
    "            \n",
    "            # 将点的id转换，映射到从0开始的。info[0]是node old id,\n",
    "            node_map[info[0]] = i\n",
    "            # info[-1]是label, 字符串, 比如'Neural_Networks'和'Rule_Learning', 转换成int来表示类\n",
    "            if not info[-1] in label_map:\n",
    "                label_map[info[-1]] = len(label_map)\n",
    "            labels[i] = label_map[info[-1]]\n",
    "\n",
    "    # 读图存储成邻接表\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(\"./cora/cora.cites\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            # 每一行是一条边\n",
    "            info = line.strip().split()\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    # 举例：(a, b) (a, c) (a, d) (b, c) (b, d)\n",
    "    # 存储后 {a: set(b, c, d), b: set(a, c, d), c: set(a, b), d: set(a, b)}\n",
    "    return feat_data, labels, adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0add372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cora():\n",
    "    # 随机数设置seed(种子)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    # cora数据集点数\n",
    "    num_nodes = 2708\n",
    "    # 加载cora数据集, 分别是\n",
    "    # feat_data: 特征\n",
    "    # labels: 标签\n",
    "    # adj_lists: 邻接表，dict (key: node, value: neighbors set)\n",
    "    feat_data, labels, adj_lists = load_cora()\n",
    "    # 设置输入的input features矩阵X的维度 = 点的数量 * 特征维度\n",
    "    features = nn.Embedding(2708, 1433)\n",
    "    # 为矩阵X赋值，参数不更新\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "    # features.cuda()\n",
    "\n",
    "    # 一共两层GNN layer\n",
    "    # 第一层GNN\n",
    "    # 以mean的方式聚合邻居, algorithm 1 line 4\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    # 将自身和聚合邻居的向量拼接后送入到神经网络(可选是否只用聚合邻居的信息来表示), algorithm 1 line 5\n",
    "    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "\n",
    "    # 第二层GNN\n",
    "    # 将第一层的GNN输出作为输入传进去\n",
    "    # 这里面.t()表示转置，是因为Encoder class的输出维度为embed_dim * nodes\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    # enc1.embed_dim = 128, 变换后的维度还是128\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "            base_model=enc1, gcn=True, cuda=False)\n",
    "\n",
    "    # 采样的邻居点的数量\n",
    "    enc1.num_samples = 5\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    # 7分类问题\n",
    "    # enc2是经过两层GNN layer时候得到的 node embedding/features\n",
    "    graphsage = SupervisedGraphSage(7, enc2)\n",
    "    # graphsage.cuda()\n",
    "\n",
    "    # 目的是打乱节点顺序\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "\n",
    "    # 划分测试集、验证集、训练集\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    # 用SGD的优化，设置学习率\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    # 记录每个batch训练时间\n",
    "    times = []\n",
    "    # 共训练100个batch\n",
    "    for batch in range(100):\n",
    "        # 取256个nodes作为一个batch\n",
    "        batch_nodes = train[:256]\n",
    "        # 打乱训练集的顺序，使下次迭代batch随机\n",
    "        random.shuffle(train)\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        # 这个是SupervisedGraphSage里面定义的cross entropy loss\n",
    "        loss = graphsage.loss(batch_nodes, \n",
    "                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        # 反向传播和更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        # print (batch, loss.data[0])\n",
    "        print (batch, loss.data)\n",
    "\n",
    "    # 做validation\n",
    "    val_output = graphsage.forward(val)\n",
    "    # 计算micro F1 score\n",
    "    print (\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    # 计算每个batch的平均训练时间\n",
    "    print (\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa87d640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/kh58t7hj5s99k8krgn4brl1c0000gn/T/ipykernel_47733/2510499819.py:37: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n",
      "/var/folders/rj/kh58t7hj5s99k8krgn4brl1c0000gn/T/ipykernel_47733/144133375.py:10: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n",
      "/var/folders/rj/kh58t7hj5s99k8krgn4brl1c0000gn/T/ipykernel_47733/2254303744.py:46: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  samp_neighs = [_set(_sample(to_neigh,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.9582)\n",
      "1 tensor(1.9414)\n",
      "2 tensor(1.9168)\n",
      "3 tensor(1.8915)\n",
      "4 tensor(1.8607)\n",
      "5 tensor(1.8367)\n",
      "6 tensor(1.8065)\n",
      "7 tensor(1.7766)\n",
      "8 tensor(1.7207)\n",
      "9 tensor(1.6772)\n",
      "10 tensor(1.5826)\n",
      "11 tensor(1.5639)\n",
      "12 tensor(1.5026)\n",
      "13 tensor(1.4706)\n",
      "14 tensor(1.4020)\n",
      "15 tensor(1.3103)\n",
      "16 tensor(1.2550)\n",
      "17 tensor(1.2003)\n",
      "18 tensor(1.0233)\n",
      "19 tensor(0.9690)\n",
      "20 tensor(0.9384)\n",
      "21 tensor(0.8957)\n",
      "22 tensor(0.8020)\n",
      "23 tensor(0.7801)\n",
      "24 tensor(0.7624)\n",
      "25 tensor(0.8598)\n",
      "26 tensor(0.7577)\n",
      "27 tensor(0.9079)\n",
      "28 tensor(0.9362)\n",
      "29 tensor(0.9938)\n",
      "30 tensor(0.5717)\n",
      "31 tensor(0.6399)\n",
      "32 tensor(0.5354)\n",
      "33 tensor(0.5127)\n",
      "34 tensor(0.4802)\n",
      "35 tensor(0.4303)\n",
      "36 tensor(0.4532)\n",
      "37 tensor(0.5168)\n",
      "38 tensor(0.5495)\n",
      "39 tensor(0.6705)\n",
      "40 tensor(0.5561)\n",
      "41 tensor(0.4420)\n",
      "42 tensor(0.3657)\n",
      "43 tensor(0.4114)\n",
      "44 tensor(0.3975)\n",
      "45 tensor(0.3443)\n",
      "46 tensor(0.3273)\n",
      "47 tensor(0.3100)\n",
      "48 tensor(0.3277)\n",
      "49 tensor(0.3752)\n",
      "50 tensor(0.3119)\n",
      "51 tensor(0.3798)\n",
      "52 tensor(0.3248)\n",
      "53 tensor(0.4031)\n",
      "54 tensor(0.2793)\n",
      "55 tensor(0.3573)\n",
      "56 tensor(0.3652)\n",
      "57 tensor(0.3110)\n",
      "58 tensor(0.3217)\n",
      "59 tensor(0.2694)\n",
      "60 tensor(0.3038)\n",
      "61 tensor(0.2933)\n",
      "62 tensor(0.3497)\n",
      "63 tensor(0.2439)\n",
      "64 tensor(0.2418)\n",
      "65 tensor(0.2488)\n",
      "66 tensor(0.2693)\n",
      "67 tensor(0.3153)\n",
      "68 tensor(0.2618)\n",
      "69 tensor(0.2409)\n",
      "70 tensor(0.2548)\n",
      "71 tensor(0.1955)\n",
      "72 tensor(0.2040)\n",
      "73 tensor(0.2005)\n",
      "74 tensor(0.2060)\n",
      "75 tensor(0.2081)\n",
      "76 tensor(0.2088)\n",
      "77 tensor(0.1995)\n",
      "78 tensor(0.2431)\n",
      "79 tensor(0.1899)\n",
      "80 tensor(0.1945)\n",
      "81 tensor(0.1996)\n",
      "82 tensor(0.2009)\n",
      "83 tensor(0.2109)\n",
      "84 tensor(0.1851)\n",
      "85 tensor(0.1718)\n",
      "86 tensor(0.2167)\n",
      "87 tensor(0.1153)\n",
      "88 tensor(0.1994)\n",
      "89 tensor(0.2012)\n",
      "90 tensor(0.1942)\n",
      "91 tensor(0.1847)\n",
      "92 tensor(0.1734)\n",
      "93 tensor(0.1899)\n",
      "94 tensor(0.1744)\n",
      "95 tensor(0.1892)\n",
      "96 tensor(0.1353)\n",
      "97 tensor(0.1432)\n",
      "98 tensor(0.1707)\n",
      "99 tensor(0.1823)\n",
      "Validation F1: 0.842\n",
      "Average batch time: 0.008431470394134522\n"
     ]
    }
   ],
   "source": [
    "run_cora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897ea8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
