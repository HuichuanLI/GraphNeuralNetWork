{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b15bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4162d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    # identity创建方矩阵\n",
    "    # 字典key为label的值，value为矩阵的每一行\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    # get函数得到字典key对应的value\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "    # map() 会根据提供的函数对指定序列做映射\n",
    "    # 第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表\n",
    "    #  map(lambda x: x ** 2, [1, 2, 3, 4, 5])\n",
    "    #  output:[1, 4, 9, 16, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb7299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "    \n",
    "    # content file的每一行的格式为: <paper_id> <word_attributes> <class_label>\n",
    "    # 分别对应 0, 1:-1, -1\n",
    "    # feature为第二列到倒数第二列，labels为最后一列\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "\n",
    "    # 储存为csr型稀疏矩阵\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    \n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    # cites file的每一行格式为: <cited paper ID>  <citing paper ID>\n",
    "    # 根据前面的contents与这里的cites创建图，算出edges矩阵与adj矩阵\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    \n",
    "    # 由于文件中节点并非是按顺序排列的，因此建立一个编号为0-(node_size-1)的哈希表idx_map，\n",
    "    # 哈希表中每一项为old id: number，即节点id对应的编号为number\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    \n",
    "    # edges_unordered为直接从边表文件中直接读取的结果，是一个(edge_num, 2)的数组，每一行表示一条边两个端点的idx\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    \n",
    "    # flatten：降维，返回一维数组\n",
    "    # 边的edges_unordered中存储的是端点id，要将每一项的old id换成编号number\n",
    "    # 在idx_map中以idx作为键查找得到对应节点的编号，reshape成与edges_unordered形状一样的数组\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    \n",
    "    # 根据coo矩阵性质，这一段的作用就是，网络有多少条边，邻接矩阵就有多少个1，\n",
    "    # 所以先创建一个长度为edge_num的全1数组，每个1的填充位置就是一条边中两个端点的编号，\n",
    "    # 即edges[:, 0], edges[:, 1]，矩阵的形状为(node_size, node_size)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    # 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵\n",
    "    # 将i->j与j->i中权重最大的那个, 作为无向图的节点i与节点j的边权.\n",
    "    # https://blog.csdn.net/Eric_1993/article/details/102907104\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize_features(features)\n",
    "    \n",
    "    # eye创建单位矩阵，第一个参数为行数，第二个为列数\n",
    "    # 论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
    "    # 对应公式A~=A+I_N\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    \n",
    "    # 分别构建训练集、验证集、测试集，并创建特征矩阵、标签向量和邻接矩阵的tensor，用来做模型的输入\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    # 邻接矩阵转为tensor处理\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060d4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    # https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780\n",
    "    # https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b\n",
    "    # 论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
    "    # 对每一行求和\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    # (D~)^0.5\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    # 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    # 构建对角元素为r_inv的对角矩阵\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    # 论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167515f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    # 对每一行求和\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    # 求倒数\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    # 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    # 构建对角元素为r_inv的对角矩阵\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    # 用对角矩阵与原始矩阵的点积起到标准化的作用，原始矩阵中每一行元素都会与对应的r_inv相乘，最终相当于除以了sum\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b3170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    # 使用type_as(tesnor)将张量转换为给定类型的张量\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    # 记录等于preds的label eq:equal\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c439052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e24124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # 学习因子\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        \n",
    "        # 建立都是0的矩阵，大小为（输入维度，输出维度）\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        # xavier初始化\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        # 这里的self.a,对应的是论文里的向量a，故其维度大小应该为(2*out_features, 1)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        # h.shape: torch.Size([2708, 8]) 8是label的个数\n",
    "        Wh = torch.mm(h, self.W)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        \n",
    "        # 即论文里的eij\n",
    "        # squeeze除去维数为1的维度\n",
    "        # [2708, 2708, 16]与[16, 1]相乘再除去维数为1的维度，故其维度为[2708,2708],与领接矩阵adj的维度一样\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        \n",
    "        # mask-attention\n",
    "        \n",
    "        # 维度大小与e相同，所有元素都是-9*10的15次方\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        \n",
    "        # 故adj的领接矩阵的大小为[2708, 2708] (归一化处理之后的)\n",
    "        # 故当adj>0，即两结点有边，则用gat构建的矩阵e，若adj=0,则另其为一个很大的负数，这么做的原因是进行softmax时，这些数就会接近于0了\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        \n",
    "        # 对应论文公式3，attention就是公式里的a_ij\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        N = Wh.size()[0] # number of nodes\n",
    "\n",
    "        # Below, two matrices are created that contain embeddings in their rows in different orders.\n",
    "        # (e stands for embedding)\n",
    "        # These are the rows of the first matrix (Wh_repeated_in_chunks): \n",
    "        # e1, e1, ..., e1,            e2, e2, ..., e2,            ..., eN, eN, ..., eN\n",
    "        # '-------------' -> N times  '-------------' -> N times       '-------------' -> N times\n",
    "        \n",
    "        # \n",
    "        # These are the rows of the second matrix (Wh_repeated_alternating): \n",
    "        # e1, e2, ..., eN, e1, e2, ..., eN, ..., e1, e2, ..., eN \n",
    "        # '----------------------------------------------------' -> N times\n",
    "        \n",
    "        # https://www.jianshu.com/p/a2102492293a\n",
    "        \n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "        # Wh_repeated_in_chunks.shape == Wh_repeated_alternating.shape == (N * N, out_features)\n",
    "\n",
    "        # The all_combination_matrix, created below, will look like this (|| denotes concatenation):\n",
    "        # e1 || e1\n",
    "        # e1 || e2\n",
    "        # e1 || e3\n",
    "        # ...\n",
    "        # e1 || eN\n",
    "        # e2 || e1\n",
    "        # e2 || e2\n",
    "        # e2 || e3\n",
    "        # ...\n",
    "        # e2 || eN\n",
    "        # ...\n",
    "        # eN || e1\n",
    "        # eN || e2\n",
    "        # eN || e3\n",
    "        # ...\n",
    "        # eN || eN\n",
    "\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        # all_combinations_matrix.shape == (N * N, 2 * out_features)\n",
    "\n",
    "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4eddbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0354965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c30a8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8776f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # 输入到隐藏层\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        \n",
    "        # multi-head隐藏层到输出\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # 这里的torch.cat即公式（5）中的||\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f873bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
    "                                             nclass, \n",
    "                                             dropout=dropout, \n",
    "                                             alpha=alpha, \n",
    "                                             concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d32f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "# 禁用CUDA训练\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "# 在训练通过期间验证\n",
    "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "# sparse GAT选项\n",
    "parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "# 随机种子\n",
    "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "# 要训练的epoch数\n",
    "# parser.add_argument('--epochs', type=int, default=10000, help='Number of epochs to train.')\n",
    "parser.add_argument('--epochs', type=int, default=2, help='Number of epochs to train.')\n",
    "# 最初的学习率\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "# 权重衰减（参数L2损失）\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "# 隐藏层单元数量\n",
    "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
    "# 多头注意力机制\n",
    "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
    "# dropout率（1-保持概率)\n",
    "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
    "# Leaky ReLU参数\n",
    "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "\n",
    "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# 产生随机种子，以使得结果是确定的\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b6e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "# 模型和优化器\n",
    "\n",
    "# GAT模型\n",
    "# nfeat输入单元数，shape[1]表示特征矩阵的维度数（列数）\n",
    "# nhid中间层单元数量\n",
    "# nclass输出单元数，即样本标签数=样本标签最大值+1\n",
    "# dropout参数\n",
    "# nheads多头注意力数量\n",
    "sparse = False\n",
    "if sparse:\n",
    "    model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "else:\n",
    "    model = GAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "    \n",
    "# 构造一个优化器对象Optimizer，用来保存当前的状态，并能够根据计算得到的梯度来更新参数\n",
    "# Adam优化器\n",
    "# lr学习率\n",
    "# weight_decay权重衰减（L2惩罚）\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)\n",
    "\n",
    "# 如果使用GUP则执行这里，数据写入cuda，便于后续加速\n",
    "cuda = False\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c9a2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(epoch):\n",
    "    # 返回当前时间\n",
    "    t = time.time()\n",
    "    \n",
    "    # train的时候使用dropout, 测试的时候不使用dropout\n",
    "    # pytorch里面eval()固定整个网络参数，没有dropout\n",
    "    \n",
    "    # 固定语句，主要针对启用BatchNormalization和Dropout\n",
    "    model.train()\n",
    "    \n",
    "    # 把梯度置零，也就是把loss关于weight的导数变成0\n",
    "    optimizer.zero_grad()\n",
    "    # 执行GAT中的forward前向传播\n",
    "    output = model(features, adj)\n",
    "    # 最大似然/log似然损失函数，idx_train是140(0~139)\n",
    "    # nll_loss: negative log likelihood loss\n",
    "    # https://www.cnblogs.com/marsggbo/p/10401215.html\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    # 准确率\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    # 反向传播\n",
    "    loss_train.backward()\n",
    "    # 梯度下降，更新值\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    # 是否在训练期间进行验证\n",
    "    if not args.fastmode:\n",
    "        # 固定语句，主要针对不启用BatchNormalization和Dropout\n",
    "        model.eval() \n",
    "        # 前向传播\n",
    "        output = model(features, adj)\n",
    "    \n",
    "    # 最大似然/log似然损失函数，idx_val是300(200~499)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    # 准确率\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "    # 正在迭代的epoch数\n",
    "    # 训练集损失函数值\n",
    "    # 训练集准确率\n",
    "    # 验证集损失函数值\n",
    "    # 验证集准确率\n",
    "    # 运行时间\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed87f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9484 acc_train: 0.0857 loss_val: 1.9404 acc_val: 0.2867 time: 4.2909s\n",
      "Epoch: 0002 loss_train: 1.9341 acc_train: 0.2286 loss_val: 1.9303 acc_val: 0.4333 time: 3.1546s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 7.5623s\n",
      "Loading 1th epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "# epoch数\n",
    "for epoch in range(args.epochs):\n",
    "    # 训练\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "        \n",
    "    # 连续patience次数效果不变好则提前停止\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "# 已用总时间\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0941c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 1.9327 accuracy= 0.3430\n"
     ]
    }
   ],
   "source": [
    "# 定义测试函数，相当于对已有的模型在测试集上运行对应的loss与accuracy\n",
    "def compute_test():\n",
    "    # 固定语句，主要针对不启用BatchNormalization和Dropout\n",
    "    model.eval()\n",
    "    # 前向传播\n",
    "    output = model(features, adj)\n",
    "    # 最大似然/log似然损失函数，idx_test是1000(500~1499)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    # 准确率\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "#     print(\"Test set results:\",\n",
    "#           \"loss= {:.4f}\".format(loss_test.data[0]),\n",
    "#           \"accuracy= {:.4f}\".format(acc_test.data[0]))\n",
    "\n",
    "    # 测试集损失函数值\n",
    "    # 测试集的准确率\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data))\n",
    "# Testing\n",
    "compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa2fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
