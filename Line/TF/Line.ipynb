{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T10:22:24.454800Z",
     "start_time": "2021-08-28T10:22:24.444380Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @Time : 2021/8/28 5:22 下午\n",
    "# @Author : huichuan LI\n",
    "# @File : Line.py\n",
    "# @Software: PyCharm\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Line(keras.Model):\n",
    "    def __init__(self, size, embed_dim=128, order=1):\n",
    "        super(Line, self).__init__()\n",
    "\n",
    "        assert order in [1, 2], print(\"Order should either be int(1) or int(2)\")\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.order = order\n",
    "        self.nodes_embeddings = keras.layers.Embedding(size, embed_dim,\n",
    "                                                       embeddings_initializer=keras.initializers.RandomNormal(0., 0.1),\n",
    "                                                       )\n",
    "\n",
    "        if order == 2:\n",
    "            self.contextnodes_embeddings = keras.layers.Embedding(size, embed_dim,\n",
    "                                                                  embeddings_initializer=keras.initializers.RandomNormal(\n",
    "                                                                      0., 0.1),\n",
    "                                                                  )\n",
    "\n",
    "    def call(self, v_i, v_j, negsamples):\n",
    "\n",
    "        v_i = self.nodes_embeddings(v_i)\n",
    "\n",
    "        if self.order == 2:\n",
    "            v_j = self.contextnodes_embeddings(v_j)\n",
    "            negativenodes = -self.contextnodes_embeddings(negsamples)\n",
    "\n",
    "        else:\n",
    "            v_j = self.nodes_embeddings(v_j)\n",
    "            negativenodes = -self.nodes_embeddings(negsamples)\n",
    "\n",
    "        mulpositivebatch = tf.multiply(v_i, v_j)\n",
    "        positivebatch = tf.keras.activations.sigmoid(tf.reduce_sum(mulpositivebatch, axis=1))\n",
    "\n",
    "        mulnegativebatch = tf.multiply(\n",
    "            tf.reshape(len(v_i), 1, self.embed_dim), negativenodes)\n",
    "        negativebatch = tf.reduce_sum(\n",
    "            tf.keras.activations.sigmoid(\n",
    "                tf.reduce_sum(mulnegativebatch, dim=2)\n",
    "            ),\n",
    "            axis=1)\n",
    "        loss = positivebatch + negativebatch\n",
    "        return -tf.reduce_mean(loss)\n",
    "    \n",
    "\n",
    "    def train(self, v_i, v_j, negsamples):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss  = self.call(self, v_i, v_j, negsamples)\n",
    "        gradients = tape.gradient(loss, self.policygrad_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.policygrad_model.trainable_variables))\n",
    "        \n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T09:39:32.320374Z",
     "start_time": "2021-08-28T09:39:32.316802Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "weighted_edge_list = []\n",
    "\n",
    "with open('../../Graph/karate.edgelist', \"r\") as graphfile:\n",
    "    for l in graphfile:\n",
    "        s = l.strip().split(\" \")\n",
    "        weighted_edge_list.append([s[0], s[1], '1'])\n",
    "                       \n",
    "# with open('../Graph/karate.edgelist', \"w+\") as w:\n",
    "#     for l in weighted_edge_list:\n",
    "#         w.writelines(\" \".join([str(s) for s in l]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T09:42:46.479188Z",
     "start_time": "2021-08-28T09:42:46.458495Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from decimal import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "class VoseAlias(object):\n",
    "    \"\"\"\n",
    "    构建alias table,达到O(1)的采样效率\n",
    "    Adding a few modifs to https://github.com/asmith26/Vose-Alias-Method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dist):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        (VoseAlias, dict) -> NoneType\n",
    "        \"\"\"\n",
    "        self.dist = dist\n",
    "        self.alias_initialisation()\n",
    "\n",
    "    def alias_initialisation(self):\n",
    "        \"\"\"\n",
    "        Construct probability and alias tables for the distribution.\n",
    "        \"\"\"\n",
    "        # Initialise variables\n",
    "        n = len(self.dist)\n",
    "        # 概率表\n",
    "        self.table_prob = {}   # probability table\n",
    "        # 替身表\n",
    "        self.table_alias = {}  # alias table\n",
    "        # 乘以n的概率表\n",
    "        scaled_prob = {}       # scaled probabilities\n",
    "        # 存储概率值小于1的\n",
    "        small = []             # stack for probabilities smaller that 1\n",
    "        # 存储概率值大于1的\n",
    "        large = []             # stack for probabilities greater than or equal to 1\n",
    "\n",
    "        # Construct and sort the scaled probabilities into their appropriate stacks\n",
    "        # 将各个概率分成两组，一组的概率值大于1，另一组的概率值小于1\n",
    "        print(\"1/2. Building and sorting scaled probabilities for alias table...\")\n",
    "        for o, p in tqdm(self.dist.items()):\n",
    "            scaled_prob[o] = Decimal(p) * n\n",
    "\n",
    "            if scaled_prob[o] < 1:\n",
    "                small.append(o)\n",
    "            else:\n",
    "                large.append(o)\n",
    "\n",
    "        print(\"2/2. Building alias table...\")\n",
    "        # Construct the probability and alias tables\n",
    "        # 使用贪心算法，将概率值小于1的不断填满\n",
    "        while small and large:\n",
    "            s = small.pop()\n",
    "            l = large.pop()\n",
    "\n",
    "            self.table_prob[s] = scaled_prob[s]\n",
    "            self.table_alias[s] = l\n",
    "            # 更新概率值\n",
    "            scaled_prob[l] = (scaled_prob[l] + scaled_prob[s]) - Decimal(1)\n",
    "\n",
    "            if scaled_prob[l] < 1:\n",
    "                small.append(l)\n",
    "            else:\n",
    "                large.append(l)\n",
    "\n",
    "        # The remaining outcomes (of one stack) must have probability 1\n",
    "        # 当两方不全有元素时，仅有一方有元素的也全为1\n",
    "        while large:\n",
    "            self.table_prob[large.pop()] = Decimal(1)\n",
    "\n",
    "        while small:\n",
    "            self.table_prob[small.pop()] = Decimal(1)\n",
    "        self.listprobs = list(self.table_prob)\n",
    "\n",
    "    def alias_generation(self):\n",
    "        \"\"\"\n",
    "        Yields a random outcome from the distribution.\n",
    "        \"\"\"\n",
    "        # Determine which column of table_prob to inspect\n",
    "        col = random.choice(self.listprobs)\n",
    "        # Determine which outcome to pick in that column\n",
    "        # 取自己 \n",
    "        if self.table_prob[col] >= random.uniform(0, 1):\n",
    "            return col\n",
    "        # 取替身，即取alias table存的节点\n",
    "        else:\n",
    "            return self.table_alias[col]\n",
    "\n",
    "    def sample_n(self, size):\n",
    "        \"\"\"\n",
    "        调用alias_generation一共n次，采样n个nodes\n",
    "        Yields a sample of size n from the distribution, and print the results to stdout.\n",
    "        \"\"\"\n",
    "        for i in range(size):\n",
    "            yield self.alias_generation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T09:42:47.016725Z",
     "start_time": "2021-08-28T09:42:47.007617Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def makeDist(graphpath, power=0.75):\n",
    "    # 读图函数\n",
    "    # 初始化词典\n",
    "    edgedistdict = collections.defaultdict(int)\n",
    "    nodedistdict = collections.defaultdict(int)\n",
    "\n",
    "    weightsdict = collections.defaultdict(int)\n",
    "    nodedegrees = collections.defaultdict(int)\n",
    "\n",
    "    weightsum = 0\n",
    "    negprobsum = 0\n",
    "    # 统计图一共有多少条边\n",
    "    nlines = 0\n",
    "    \n",
    "    with open(graphpath, \"r\") as graphfile:\n",
    "        for l in graphfile:\n",
    "            nlines += 1\n",
    "\n",
    "    print(\"Reading edgelist file...\")\n",
    "    maxindex = 0\n",
    "    with open(graphpath, \"r\") as graphfile:\n",
    "        # tqdm能展示for循环进度百分比\n",
    "        for l in tqdm(graphfile, total=nlines):\n",
    "            # 点i,点j,weight\n",
    "            line = [int(i) for i in l.replace(\"\\n\", \"\").split(\" \")]\n",
    "            node1, node2, weight = line[0], line[1], line[2]\n",
    "            \n",
    "            # 后面会做归一化，存的是归一化的边-权重和点-出度\n",
    "            edgedistdict[tuple([node1, node2])] = weight\n",
    "            nodedistdict[node1] += weight\n",
    "            \n",
    "            # 不再做处理，存的是边-权重，点-出度\n",
    "            weightsdict[tuple([node1, node2])] = weight\n",
    "            nodedegrees[node1] += weight\n",
    "            \n",
    "            # weightsum存的是全图所有边的边权和，论文公式（2）中用到的1st相似度真实值\n",
    "            weightsum += weight\n",
    "            negprobsum += np.power(weight, power)\n",
    "            \n",
    "            # maxindex记录图中最大顶点index\n",
    "            if node1 > maxindex:\n",
    "                maxindex = node1\n",
    "            elif node2 > maxindex:\n",
    "                maxindex = node2\n",
    "    for node, outdegree in nodedistdict.items():\n",
    "        nodedistdict[node] = np.power(outdegree, power) / negprobsum\n",
    "\n",
    "    for edge, weight in edgedistdict.items():\n",
    "        edgedistdict[edge] = weight / weightsum\n",
    "\n",
    "    return edgedistdict, nodedistdict, weightsdict, nodedegrees, maxindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T09:42:47.438605Z",
     "start_time": "2021-08-28T09:42:47.434663Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def negSampleBatch(sourcenode, targetnode, negsamplesize, weights,\n",
    "                   nodedegrees, nodesaliassampler, t=10e-3):\n",
    "    \"\"\"\n",
    "    For generating negative samples.\n",
    "    \"\"\"\n",
    "    negsamples = 0\n",
    "    while negsamples < negsamplesize:\n",
    "        # nodesaliassampler是实现alias building的VoseAlias类，这里采样点\n",
    "        samplednode = nodesaliassampler.sample_n(1)\n",
    "        # 如果采样出source或target均跳过\n",
    "        if (samplednode == sourcenode) or (samplednode == targetnode):\n",
    "            continue\n",
    "        # 输出负样本点，一共negsamplesize个点\n",
    "        else:\n",
    "            negsamples += 1\n",
    "            yield samplednode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T09:42:47.834223Z",
     "start_time": "2021-08-28T09:42:47.829919Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def makeData(samplededges, negsamplesize, weights, nodedegrees, nodesaliassampler):\n",
    "    for e in samplededges:\n",
    "        sourcenode, targetnode = e[0], e[1]\n",
    "        negnodes = []\n",
    "        # 采样出negsamplesize个负样本点\n",
    "        for negsample in negSampleBatch(sourcenode, targetnode, negsamplesize,\n",
    "                                        weights, nodedegrees, nodesaliassampler):\n",
    "            for node in negsample:\n",
    "                negnodes.append(node)\n",
    "        # 格式是(node i, node j, negative nodes...)\n",
    "        yield [e[0], e[1]] + negnodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T10:10:40.816265Z",
     "start_time": "2021-08-28T10:10:40.810399Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 83586.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading edgelist file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "negativepower = 0.75\n",
    "\n",
    "edgedistdict, nodedistdict, weights, nodedegrees, maxindex = makeDist(\n",
    "'../../Graph/weighted.karate.edgelist', negativepower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T10:13:40.051198Z",
     "start_time": "2021-08-28T10:13:40.044507Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 160757.30it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 111962.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2. Building and sorting scaled probabilities for alias table...\n",
      "2/2. Building alias table...\n",
      "1/2. Building and sorting scaled probabilities for alias table...\n",
      "2/2. Building alias table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 构建alias table,达到O(1)的采样效率\n",
    "edgesaliassampler = VoseAlias(edgedistdict)\n",
    "nodesaliassampler = VoseAlias(nodedistdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T10:15:12.307897Z",
     "start_time": "2021-08-28T10:15:12.287935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "# 按batchsize将训练样本分组\n",
    "\n",
    "opt = keras.optimizers.Adam(0.01)\n",
    "\n",
    "batchrange = int(len(edgedistdict) / 5)\n",
    "print(maxindex)\n",
    "# line.py中的nn.Module类\n",
    "line = Line(maxindex + 1, embed_dim=128, order=2)\n",
    "# # # SGD算法优化模型\n",
    "# opt = optim.SGD(line.parameters(), lr=args.learning_rate,\n",
    "#             momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-28T10:22:02.652789Z",
     "start_time": "2021-08-28T10:22:02.635054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "tf.Tensor(\n",
      "[[ 1  4 29  5 19 23 24]\n",
      " [ 5  7  5 23 32 33 28]\n",
      " [ 1 22 30 16 23  2 31]\n",
      " [ 1  3 15 21 32 25 23]\n",
      " [ 3 14 24 10  2  1 21]], shape=(5, 7), dtype=int32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-29-c2344937d6f0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0;31m# 在做BP之前将gradients置0因为是累加的\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0;31m# Line模型实现部分\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv_i\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv_j\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnegsamples\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     32\u001B[0m         \u001B[0;31m# 计算梯度\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0;31m# 根据梯度值更新参数值\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "lossdata = {\"it\": [], \"loss\": []}\n",
    "it = 0\n",
    "helper = 0\n",
    "\n",
    "# 共训练epoch次数\n",
    "for epoch in range(20):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    # 每次训练组数：batchsize\n",
    "    for b in range(batchrange):\n",
    "        # edgesaliassampler是实现alias building的VoseAlias类，这里采样出batchsize条边\n",
    "        samplededges = edgesaliassampler.sample_n(5)\n",
    "        # makeData是utils.py中的函数，为每条边采样出K条负样本边\n",
    "        # 每一条格式是(node i, node j, negative nodes...)\n",
    "        batch = list(makeData(samplededges, 5, weights, nodedegrees,\n",
    "                              nodesaliassampler))\n",
    "        # 转换成tensor格式\n",
    "        batch = tf.convert_to_tensor(batch)\n",
    "        if helper == 0:\n",
    "            print (batch)\n",
    "            helper = 1\n",
    "        # 第0列\n",
    "        v_i = batch[:, 0]\n",
    "        # 第1列\n",
    "        v_j = batch[:, 1]\n",
    "        # 第2列-最后列\n",
    "        \n",
    "        negsamples = batch[:, 2:]\n",
    "        # 在做BP之前将gradients置0因为是累加的\n",
    "        # Line模型实现部分\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss  = self.call(self, v_i, v_j, negsamples)\n",
    "        gradients = tape.gradient(loss, self.policygrad_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, l.trainable_variables))\n",
    "        # 计算梯度\n",
    "        # 根据梯度值更新参数值\n",
    "        print(loss)\n",
    "        break\n",
    "        lossdata[\"loss\"].append(loss.item())\n",
    "        lossdata[\"it\"].append(it)\n",
    "        it += 1\n",
    "\n",
    "# print(\"\\nDone training, saving model to {}\".format(args.save_path))\n",
    "# torch.save(line, \"{}\".format(args.save_path))\n",
    "\n",
    "# print(\"Saving loss data at {}\".format(args.lossdata_path))\n",
    "# with open(args.lossdata_path, \"wb\") as ldata:\n",
    "#     pickle.dump(lossdata, ldata)\n",
    "# # sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}